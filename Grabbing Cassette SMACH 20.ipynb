{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a863b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takeshi STATE MACHINE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[ WARN] [1663953316.172067675]: Link hand_l_finger_vacuum_frame has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953316.175872255]: Link head_l_stereo_camera_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953316.176041191]: Link head_r_stereo_camera_link has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953316.177058080]: Group state 'neutral' doesn't specify all group joints in group 'arm'. wrist_ft_sensor_frame_joint is missing.\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953316.177159454]: Group state 'go' doesn't specify all group joints in group 'arm'. wrist_ft_sensor_frame_joint is missing.\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953327.352324464]: Kinematics solver doesn't support #attempts anymore, but only a timeout.\n",
      "Please remove the parameter '/robot_description_kinematics/arm/kinematics_solver_attempts' from your configuration.\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953332.371730502]: IK plugin for group 'whole_body' relies on deprecated API. Please implement initialize(RobotModel, ...).\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953332.927528811]: IK plugin for group 'whole_body_weighted' relies on deprecated API. Please implement initialize(RobotModel, ...).\u001b[0m\n",
      "\u001b[33m[ WARN] [1663953333.467099879]: IK plugin for group 'whole_body_light' relies on deprecated API. Please implement initialize(RobotModel, ...).\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ DEBUG ] : Adding state (INITIAL, <__main__.Initial object at 0x7ff0682d0040>, {'failed': 'INITIAL', 'succ': 'FIND_AR_MARKER', 'tries': 'END'})\n",
      "[ DEBUG ] : Adding state 'INITIAL' to the state machine.\n",
      "[ DEBUG ] : State 'INITIAL' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR INITIAL: {'failed': 'INITIAL', 'succ': 'FIND_AR_MARKER', 'tries': 'END'}\n",
      "[ DEBUG ] : Adding state (FIND_AR_MARKER, <__main__.Find_AR_marker object at 0x7ff0682d04f0>, {'failed': 'END', 'succ': 'AR_ALIGNMENT', 'tries': 'FIND_AR_MARKER'})\n",
      "[ DEBUG ] : Adding state 'FIND_AR_MARKER' to the state machine.\n",
      "[ DEBUG ] : State 'FIND_AR_MARKER' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR FIND_AR_MARKER: {'failed': 'END', 'succ': 'AR_ALIGNMENT', 'tries': 'FIND_AR_MARKER'}\n",
      "[ DEBUG ] : Adding state (AR_ALIGNMENT, <__main__.AR_alignment object at 0x7ff0682d0430>, {'failed': 'AR_ALIGNMENT', 'succ': 'PRE_GRASP_POSE', 'tries': 'AR_ALIGNMENT'})\n",
      "[ DEBUG ] : Adding state 'AR_ALIGNMENT' to the state machine.\n",
      "[ DEBUG ] : State 'AR_ALIGNMENT' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR AR_ALIGNMENT: {'failed': 'AR_ALIGNMENT', 'succ': 'PRE_GRASP_POSE', 'tries': 'AR_ALIGNMENT'}\n",
      "[ DEBUG ] : Adding state (PRE_GRASP_POSE, <__main__.Pre_grasp_pose object at 0x7ff0682d04c0>, {'failed': 'RIGHT_SHIFT', 'succ': 'AR_ADJUSTMENT', 'tries': 'PRE_GRASP_POSE'})\n",
      "[ DEBUG ] : Adding state 'PRE_GRASP_POSE' to the state machine.\n",
      "[ DEBUG ] : State 'PRE_GRASP_POSE' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR PRE_GRASP_POSE: {'failed': 'RIGHT_SHIFT', 'succ': 'AR_ADJUSTMENT', 'tries': 'PRE_GRASP_POSE'}\n",
      "[ DEBUG ] : Adding state (RIGHT_SHIFT, <__main__.Right_shift object at 0x7ff0682d01c0>, {'failed': 'RIGHT_SHIFT', 'succ': 'PRE_GRASP_POSE', 'tries': 'PRE_GRASP_POSE'})\n",
      "[ DEBUG ] : Adding state 'RIGHT_SHIFT' to the state machine.\n",
      "[ DEBUG ] : State 'RIGHT_SHIFT' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR RIGHT_SHIFT: {'failed': 'RIGHT_SHIFT', 'succ': 'PRE_GRASP_POSE', 'tries': 'PRE_GRASP_POSE'}\n",
      "[ DEBUG ] : Adding state (AR_ADJUSTMENT, <__main__.AR_adjustment object at 0x7ff0682d0550>, {'failed': 'END', 'succ': 'COLOR_ADJUSTMENT', 'tries': 'COLOR_ADJUSTMENT'})\n",
      "[ DEBUG ] : Adding state 'AR_ADJUSTMENT' to the state machine.\n",
      "[ DEBUG ] : State 'AR_ADJUSTMENT' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR AR_ADJUSTMENT: {'failed': 'END', 'succ': 'COLOR_ADJUSTMENT', 'tries': 'COLOR_ADJUSTMENT'}\n",
      "[ DEBUG ] : Adding state (COLOR_ADJUSTMENT, <__main__.Color_adjustment object at 0x7ff0682d05b0>, {'failed': 'END', 'succ': 'GRASP_TABLE', 'tries': 'END'})\n",
      "[ DEBUG ] : Adding state 'COLOR_ADJUSTMENT' to the state machine.\n",
      "[ DEBUG ] : State 'COLOR_ADJUSTMENT' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR COLOR_ADJUSTMENT: {'failed': 'END', 'succ': 'GRASP_TABLE', 'tries': 'END'}\n",
      "[ DEBUG ] : Adding state (GRASP_TABLE, <__main__.Grasp_table object at 0x7ff0682d0610>, {'failed': 'END', 'succ': 'POST_GRASP_POSE', 'tries': 'GRASP_TABLE'})\n",
      "[ DEBUG ] : Adding state 'GRASP_TABLE' to the state machine.\n",
      "[ DEBUG ] : State 'GRASP_TABLE' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR GRASP_TABLE: {'failed': 'END', 'succ': 'POST_GRASP_POSE', 'tries': 'GRASP_TABLE'}\n",
      "[ DEBUG ] : Adding state (POST_GRASP_POSE, <__main__.Post_grasp_pose object at 0x7ff0682d0670>, {'failed': 'END', 'succ': 'END', 'tries': 'GRASP_TABLE'})\n",
      "[ DEBUG ] : Adding state 'POST_GRASP_POSE' to the state machine.\n",
      "[ DEBUG ] : State 'POST_GRASP_POSE' is missing transitions: {}\n",
      "[ DEBUG ] : TRANSITIONS FOR POST_GRASP_POSE: {'failed': 'END', 'succ': 'END', 'tries': 'GRASP_TABLE'}\n",
      "[  INFO ] : State machine starting in initial state 'INITIAL' with userdata: \n",
      "\t[]\n",
      "[INFO] [1663953398.234949]: STATE : robot neutral pose\n",
      "Try 0 of 5 attepmpts\n",
      "[  INFO ] : State machine transitioning 'INITIAL':'succ'-->'FIND_AR_MARKER'\n",
      "[INFO] [1663953414.455014]: State : Find AR marker \n",
      "Try 0 of 5 attepmpts\n",
      "0.77671743621045\n",
      "0.77671743621045\n",
      "0.77671743621045\n",
      "0.7501423202657121\n",
      "0.7501423202657121\n",
      "0.7501423202657121\n",
      "0.7501423202657121\n",
      "0.7501423202657121\n",
      "0.7059117427795385\n",
      "0.7059117427795385\n",
      "0.6747061393068194\n",
      "0.6747061393068194\n",
      "0.6481172909993557\n",
      "0.6413468381773741\n",
      "0.6203725863703031\n",
      "0.6203725863703031\n",
      "0.6011850909034623\n",
      "0.6011850909034623\n",
      "0.5785919506649995\n",
      "0.5785919506649995\n",
      "0.5448472198986833\n",
      "0.5446427401001365\n",
      "0.5361132939051811\n",
      "0.5167659766069523\n",
      "0.49684805465645304\n",
      "0.4892619803293431\n",
      "0.4729749836729367\n",
      "0.4642470013125439\n",
      "0.44990620201508347\n",
      "[  INFO ] : State machine transitioning 'FIND_AR_MARKER':'succ'-->'AR_ALIGNMENT'\n",
      "[INFO] [1663953448.845874]: State : pre grasp pose \n",
      "Try 0 of 5 attepmpts\n",
      "1.608682028389803\n",
      "1.558214428709536\n",
      "1.0205578132680557\n",
      "1.0504905699084097\n",
      "0.8956248783653685\n",
      "0.8626643724496104\n",
      "0.49950693769957155\n",
      "0.6094100226278151\n",
      "0.6050446039367458\n",
      "0.6050446039367458\n",
      "0.3333277050719241\n",
      "0.3333277050719241\n",
      "0.2676296202070938\n",
      "0.2687142350528551\n",
      "0.2166284936766263\n",
      "0.06790670239704366\n",
      "[  INFO ] : State machine transitioning 'AR_ALIGNMENT':'succ'-->'PRE_GRASP_POSE'\n",
      "[INFO] [1663953470.196106]: State : pre grasp pose \n",
      "Try 0 of 5 attepmpts\n",
      "[  INFO ] : State machine transitioning 'PRE_GRASP_POSE':'succ'-->'AR_ADJUSTMENT'\n",
      "[INFO] [1663953480.078852]: State : GOTO_SHELF\n",
      "Try 0 of 5 attepmpts\n",
      "0.20925160433695744 -0.2733861510484965\n",
      "0.20706311175746572 -0.2732677895755636\n",
      "0.17823873391187606 -0.2470986497629108\n",
      "0.15849366249363195 -0.22122745194453958\n",
      "0.14179863875737064 -0.19358854050730212\n",
      "0.1394862241403576 -0.16348058957008132\n",
      "0.1173130175195436 -0.13966244910587028\n",
      "0.0964774425766235 -0.11098675420169873\n",
      "0.07770590609734596 -0.08102359133507803\n",
      "0.059507079035363564 -0.05327721344372732\n",
      "0.056178890854144514 -0.02939502871175409\n",
      "0.03159523969457401 -0.004917005218059101\n",
      "0.016014766013652704 -0.0030534464035270603\n",
      "[  INFO ] : State machine transitioning 'AR_ADJUSTMENT':'succ'-->'COLOR_ADJUSTMENT'\n",
      "[INFO] [1663953505.309316]: State : color adjustment\n",
      "Try 0 of 5 attepmpts\n",
      "-7.19814518531129 119.01488234543211\n",
      "-19.727959127185528 123.39400055358215\n",
      "-13.488722821121883 123.26749497533345\n",
      "-4.219472588480244 113.44500346981263\n",
      "-6.589077466342985 112.16946590156698\n",
      "-7.815124672819962 106.55323276851723\n",
      "-8.031609039328856 95.93670287570922\n",
      "-9.8643007543279 78.75662071959624\n",
      "-2.0183759590793215 71.25345268542202\n",
      "5.99004255620963 65.2237215405348\n",
      "9.587626905547666 61.697980512336954\n",
      "-22.73884572614722 53.40008525465191\n",
      "-20.574365997835315 34.946320039963354\n",
      "-8.42964038727527 -1.2237897648685703\n",
      "[  INFO ] : State machine transitioning 'COLOR_ADJUSTMENT':'succ'-->'GRASP_TABLE'\n",
      "[INFO] [1663953515.364259]: STATE : robot neutral pose\n",
      "Try 0 of 5 attepmpts\n",
      "[  INFO ] : State machine transitioning 'GRASP_TABLE':'succ'-->'POST_GRASP_POSE'\n",
      "[INFO] [1663953523.250677]: STATE : robot neutral pose\n",
      "Try 0 of 5 attepmpts\n",
      "[  INFO ] : State machine terminating 'POST_GRASP_POSE':'succ':'END'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from std_srvs.srv import Empty, Trigger, TriggerRequest\n",
    "import smach\n",
    "from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\n",
    "from geometry_msgs.msg import PoseStamped, Point , Quaternion, Twist\n",
    "from actionlib_msgs.msg import GoalStatus\n",
    "import moveit_commander\n",
    "import moveit_msgs.msg\n",
    "import tf2_ros\n",
    "from tf2_sensor_msgs.tf2_sensor_msgs import do_transform_cloud\n",
    "import controller_manager_msgs.srv\n",
    "import rospy\n",
    "import trajectory_msgs.msg\n",
    "import geometry_msgs.msg\n",
    "#from object_classification.srv import *\n",
    "from sensor_msgs.msg import Image as ImageMsg\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from tmc_msgs.msg import Voice\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from utils_notebooks import *\n",
    "from utils_takeshi import *\n",
    "\n",
    "# import hsrb_interface\n",
    "# from hsrb_interface import Robot\n",
    "# from hsrb_interface import geometry\n",
    "\n",
    "def talk(msg):\n",
    "    talker = rospy.Publisher('/talk_request', Voice, queue_size=10)\n",
    "    voice = Voice()\n",
    "    voice.language = 1\n",
    "    voice.sentence = msg\n",
    "    talker.publish(voice)\n",
    "    \n",
    "def color_segmentator(cam = 'hand',color = 'orange', plot = False):\n",
    "    if cam == 'head':\n",
    "        image = rgbd.get_image()\n",
    "    else:\n",
    "        image = hand_cam.get_image()\n",
    "    if color == 'blue':\n",
    "        umbral_bajo = (100,120,100)\n",
    "        umbral_alto = (150,220,240)\n",
    "    else:\n",
    "        umbral_bajo = (102,95,97)\n",
    "        umbral_alto = (115,255,255)\n",
    "    img_hsv = cv.cvtColor(image, cv.COLOR_RGB2HSV)\n",
    "# hacemos la mask y filtramos en la original\n",
    "    mask = cv2.inRange(img_hsv, umbral_bajo, umbral_alto)\n",
    "    res = cv2.bitwise_and(img_hsv, img_hsv, mask=mask)\n",
    "    if plot:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(res)\n",
    "        plt.show()\n",
    "    pos = []\n",
    "    pixels = cv.findNonZero(mask)\n",
    "#     print([pixels])\n",
    "    pixels = list(cv.mean(pixels))\n",
    "    pos.append(pixels[:2])\n",
    "    return pos\n",
    "\n",
    "def tf2_obj_2_arr(transf):\n",
    "    trans = []\n",
    "    rot = []\n",
    "    trans.append(transf.transform.translation.x)\n",
    "    trans.append(transf.transform.translation.y)\n",
    "    trans.append(transf.transform.translation.z)\n",
    "    rot.append(transf.transform.rotation.x)\n",
    "    rot.append(transf.transform.rotation.y)\n",
    "    rot.append(transf.transform.rotation.z)\n",
    "    rot.append(transf.transform.rotation.w)\n",
    "    return [trans, rot]\n",
    "    \n",
    "def correct_points(low_plane=0.0, high_plane=0.2):\n",
    "\n",
    "    #Corrects point clouds \"perspective\" i.e. Reference frame head is changed to reference frame map\n",
    "    data = rospy.wait_for_message('/hsrb/head_rgbd_sensor/depth_registered/rectified_points', PointCloud2)\n",
    "    np_data = ros_numpy.numpify(data)\n",
    "    \n",
    "#   new implementation to use only tf2\n",
    "    transf = tfbuff.lookup_transform('map', 'head_rgbd_sensor_gazebo_frame', rospy.Time())\n",
    "    [trans, rot] = tf2_obj_2_arr(transf)\n",
    "    \n",
    "    eu = np.asarray(tf.transformations.euler_from_quaternion(rot))\n",
    "    t = TransformStamped()\n",
    "    rot = tf.transformations.quaternion_from_euler(-eu[1], 0, 0)\n",
    "    t.header.stamp = data.header.stamp\n",
    "    \n",
    "    t.transform.rotation.x = rot[0]\n",
    "    t.transform.rotation.y = rot[1]\n",
    "    t.transform.rotation.z = rot[2]\n",
    "    t.transform.rotation.w = rot[3]\n",
    "\n",
    "    cloud_out = do_transform_cloud(data, t)\n",
    "    np_corrected = ros_numpy.numpify(cloud_out)\n",
    "    corrected = np_corrected.reshape(np_data.shape)\n",
    "\n",
    "    img = np.copy(corrected['y'])\n",
    "\n",
    "    img[np.isnan(img)] = 2\n",
    "    #img3 = np.where((img>low)&(img< 0.99*(trans[2])),img,255)\n",
    "    img3 = np.where((img>0.99*(trans[2])-high_plane)&(img< 0.99*(trans[2])-low_plane),img,255)\n",
    "    return img3\n",
    "\n",
    "def plane_seg_square_imgs(lower=500, higher=50000, reg_ly= 30, reg_hy=600, plt_images=True, low_plane=.0, high_plane=0.2):\n",
    "\n",
    "    #Segment  Plane using corrected point cloud\n",
    "    #Lower, higher = min, max area of the box\n",
    "    #reg_ly= 30,reg_hy=600    Region (low y  region high y ) Only centroids within region are accepted\n",
    "    \n",
    "    image = rgbd.get_h_image()\n",
    "    iimmg = rgbd.get_image()\n",
    "    points_data = rgbd.get_points()\n",
    "    img = np.copy(image)\n",
    "    img3 = correct_points(low_plane,high_plane)\n",
    "    \n",
    "#     cv2 on python 3\n",
    "    contours, hierarchy = cv2.findContours(img3.astype('uint8'),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    i = 0\n",
    "    cents = []\n",
    "    points = []\n",
    "    images = []\n",
    "    for i, contour in enumerate(contours):\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > lower and area < higher :\n",
    "            M = cv2.moments(contour)\n",
    "            # calculate x,y coordinate of center\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "            boundRect = cv2.boundingRect(contour)\n",
    "            #just for drawing rect, dont waste too much time on this\n",
    "            img = cv2.rectangle(img,(boundRect[0], boundRect[1]),(boundRect[0]+boundRect[2], boundRect[1]+boundRect[3]), (0,0,0), 2)\n",
    "            # calculate moments for each contour\n",
    "            if (cY > reg_ly and cY < reg_hy  ):\n",
    "                image_aux = iimmg[boundRect[1]:boundRect[1]+max(boundRect[2],boundRect[3]),boundRect[0]:boundRect[0]+max(boundRect[2],boundRect[3])]\n",
    "                images.append(image_aux)\n",
    "                cv2.circle(img, (cX, cY), 5, (255, 255, 255), -1)\n",
    "                cv2.putText(img, f'centroid_{i}_{cX},{cY}',    (cX - 25, cY - 25)   ,cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)\n",
    "                #print ('cX,cY',cX,cY)\n",
    "                xyz = []\n",
    "\n",
    "                for jy in range (boundRect[0], boundRect[0] + boundRect[2]):\n",
    "                    for ix in range(boundRect[1], boundRect[1] + boundRect[3]):\n",
    "                        aux = (np.asarray((points_data['x'][ix,jy], points_data['y'][ix,jy], points_data['z'][ix,jy])))\n",
    "                        if np.isnan(aux[0]) or np.isnan(aux[1]) or np.isnan(aux[2]):\n",
    "                            'reject point'\n",
    "                        else:\n",
    "                            xyz.append(aux)\n",
    "\n",
    "                xyz = np.asarray(xyz)\n",
    "                cent = xyz.mean(axis=0)\n",
    "                cents.append(cent)\n",
    "#                 print (cent)\n",
    "                points.append(xyz)\n",
    "#             else:\n",
    "#                 print ('cent out of region... rejected')\n",
    "    sub_plt = 0\n",
    "    if plt_images:\n",
    "        for image in images:\n",
    "           \n",
    "            sub_plt += 1\n",
    "            ax = plt.subplot(5, 5, sub_plt)\n",
    "          \n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    cents = np.asarray(cents)\n",
    "    ### returns centroids found and a group of 3d coordinates that conform the centroid\n",
    "    return(cents,np.asarray(points), images)\n",
    "\n",
    "def seg_square_imgs(lower=2000, higher=50000, reg_ly=0, reg_hy=1000, reg_lx=0, reg_hx=1000, plt_images=True): \n",
    "\n",
    "#     Using kmeans for image segmentation find\n",
    "#     Lower, higher = min, max area of the box\n",
    "#     reg_ly= 30,reg_hy=600,reg_lx=0,reg_hx=1000, \n",
    "#     Region (low  x,y  region high x,y ) Only centroids within region are accepted\n",
    "    image = rgbd.get_h_image()\n",
    "    iimmg = rgbd.get_image()\n",
    "    points_data = rgbd.get_points()\n",
    "    values = image.reshape((-1,3))\n",
    "    values = np.float32(values)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER  ,1000,0.1)\n",
    "    k = 6\n",
    "    _ , labels , cc = cv2.kmeans(values, k, None, criteria, 30, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    cc = np.uint8(cc)\n",
    "    segmented_image = cc[labels.flatten()]\n",
    "    segmented_image = segmented_image.reshape(image.shape)\n",
    "    th3 = cv2.adaptiveThreshold(segmented_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    im4 = cv2.erode(th3, kernel, iterations = 4)\n",
    "    plane_mask = points_data['z']\n",
    "    cv2_img = plane_mask.astype('uint8')\n",
    "    img = im4\n",
    "    contours, hierarchy = cv2.findContours(im4.astype('uint8'),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    i = 0\n",
    "    cents = []\n",
    "    points = []\n",
    "    images = []\n",
    "    for i, contour in enumerate(contours):\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > lower and area < higher :\n",
    "            M = cv2.moments(contour)\n",
    "            # calculate x,y coordinate of center\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            boundRect = cv2.boundingRect(contour)\n",
    "            #just for drawing rect, dont waste too much time on this\n",
    "            image_aux = iimmg[boundRect[1]:boundRect[1] + max(boundRect[3],boundRect[2]),boundRect[0]:boundRect[0]+max(boundRect[3],boundRect[2])]\n",
    "            images.append(image_aux)\n",
    "            img=cv2.rectangle(img,(boundRect[0], boundRect[1]),(boundRect[0]+boundRect[2], boundRect[1]+boundRect[3]), (0,0,0), 2)\n",
    "            #img=cv2.rectangle(img,(boundRect[0], boundRect[1]),(boundRect[0]+max(boundRect[2],boundRect[3]), boundRect[1]+max(boundRect[2],boundRect[3])), (0,0,0), 2)\n",
    "            # calculate moments for each contour\n",
    "            if (cY > reg_ly and cY < reg_hy and  cX > reg_lx and cX < reg_hx   ):\n",
    "                cv2.circle(img, (cX, cY), 5, (255, 255, 255), -1)\n",
    "                cv2.putText(img, f'centroid_{i}_{cX},{cY}', (cX - 25, cY - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)\n",
    "                #print ('cX,cY',cX,cY)\n",
    "                xyz = []\n",
    "                for jy in range (boundRect[0], boundRect[0]+boundRect[2]):\n",
    "                    for ix in range(boundRect[1], boundRect[1]+boundRect[3]):\n",
    "                        aux=(np.asarray((points_data['x'][ix,jy],points_data['y'][ix,jy],points_data['z'][ix,jy])))\n",
    "                        if np.isnan(aux[0]) or np.isnan(aux[1]) or np.isnan(aux[2]):\n",
    "                            'reject point'\n",
    "                        else:\n",
    "                            xyz.append(aux)\n",
    "                xyz = np.asarray(xyz)\n",
    "                cent = xyz.mean(axis=0)\n",
    "                cents.append(cent)\n",
    "                #print (cent)\n",
    "                points.append(xyz)\n",
    "            else:\n",
    "                #print ('cent out of region... rejected')\n",
    "                images.pop()\n",
    "    sub_plt = 0\n",
    "    if plt_images:\n",
    "        for image in images:\n",
    "\n",
    "            sub_plt+=1\n",
    "            ax = plt.subplot(5, 5, sub_plt )\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "    cents=np.asarray(cents)\n",
    "    #images.append(img)\n",
    "    return(cents,np.asarray(points), images)\n",
    "\n",
    "def __manipulate_gripper(pos = 0.5, vel = 0.5, effort = 0.2):\n",
    "    grip_cmd_pub = rospy.Publisher('/hsrb/gripper_controller/command',\n",
    "                               trajectory_msgs.msg.JointTrajectory, queue_size=100)\n",
    "    traj = trajectory_msgs.msg.JointTrajectory()\n",
    "    traj.joint_names = [\"hand_motor_joint\"]\n",
    "    p = trajectory_msgs.msg.JointTrajectoryPoint()\n",
    "    p.positions = [pos]\n",
    "    p.velocities = [vel]\n",
    "    p.accelerations = []\n",
    "    p.effort = [effort]\n",
    "    p.time_from_start = rospy.Duration(1)\n",
    "    traj.points = [p]\n",
    "\n",
    "    grip_cmd_pub.publish(traj)\n",
    "\n",
    "def open_gripper(eff=0.5):\n",
    "    __manipulate_gripper(pos=1.23, vel=0.5, effort=eff)\n",
    "    \n",
    "def close_gripper(eff=0.5):\n",
    "    __manipulate_gripper(pos=-0.831, vel=-0.5, effort=-eff)\n",
    "\n",
    "def static_tf_publish(cents):\n",
    "#     Publish tfs of the centroids obtained w.r.t. head sensor frame and references them to map (static)\n",
    "    transf = tfbuff.lookup_transform('map', 'base_link', rospy.Time(0))\n",
    "    [trans, rot] = tf2_obj_2_arr(transf)\n",
    "#     closest_centroid_index=  np.argmin(np.linalg.norm(trans-cents, axis=1))##CLOSEST CENTROID\n",
    "    closest_centroid_index = 0\n",
    "    min_D_to_base = 10\n",
    "    for  i, cent  in enumerate(cents):\n",
    "        x, y, z = cent\n",
    "        if np.isnan(x) or np.isnan(y) or np.isnan(z):\n",
    "            print('nan')\n",
    "        else:\n",
    "            t = geometry_msgs.msg.TransformStamped()\n",
    "            t.header.stamp = rospy.Time.now()\n",
    "            t.header.frame_id = \"head_rgbd_sensor_link\"\n",
    "            t.child_frame_id = f'Object{i}'\n",
    "            t.transform.translation.x = x\n",
    "            t.transform.translation.y = y\n",
    "            t.transform.translation.z = z\n",
    "            t.transform.rotation.x = rot[0]\n",
    "            t.transform.rotation.y = rot[1]\n",
    "            t.transform.rotation.z = rot[2]\n",
    "            t.transform.rotation.w = rot[3]\n",
    "            broad.sendTransform(t)\n",
    "#             broad.sendTransform((x,y,z), rot, rospy.Time.now(), 'Object'+str(i), \"head_rgbd_sensor_link\")\n",
    "            rospy.sleep(0.5)\n",
    "            transf = tfbuff.lookup_transform('map', f'Object{i}', rospy.Time(0))\n",
    "            [xyz_map, cent_quat] = tf2_obj_2_arr(transf)\n",
    "            D_to_base = np.linalg.norm(np.asarray(trans)[:2] - np.asarray(xyz_map)[:2])\n",
    "            if D_to_base <= min_D_to_base:\n",
    "                min_D_to_base = D_to_base\n",
    "                closest_centroid_index = i\n",
    "                closest_centroid_height = xyz_map[2]\n",
    "            print ('Distance: base to obj - ', i, np.linalg.norm(np.asarray(trans)[:2] - np.asarray(xyz_map)[:2]))\n",
    "    i = closest_centroid_index\n",
    "    transf = tfbuff.lookup_transform('map', f'Object{i}', rospy.Time(0))\n",
    "    [xyz_map, cent_quat] = tf2_obj_2_arr(transf)\n",
    "    print('Height closest centroid map', xyz_map[2])\n",
    "    map_euler = tf.transformations.euler_from_quaternion(cent_quat)\n",
    "    rospy.sleep(.5)\n",
    "#     FIXING TF TO MAP ( ODOM REALLY)    \n",
    "    static_ts = TransformStamped()\n",
    "    static_ts.header.stamp = rospy.Time.now()\n",
    "    static_ts.header.frame_id = \"map\"\n",
    "    static_ts.child_frame_id = 'cassette'\n",
    "    static_ts.transform.translation.x = float(xyz_map[0])\n",
    "    static_ts.transform.translation.y = float(xyz_map[1])\n",
    "    static_ts.transform.translation.z = float(xyz_map[2])\n",
    "#     quat = tf.transformations.quaternion_from_euler(-euler[0],0,1.5)\n",
    "    static_ts.transform.rotation.x = 0#-quat[0]#trans.transform.rotation.x\n",
    "    static_ts.transform.rotation.y = 0#-quat[1]#trans.transform.rotation.y\n",
    "    static_ts.transform.rotation.z = 0#-quat[2]#trans.transform.rotation.z\n",
    "    static_ts.transform.rotation.w = 1#-quat[3]#trans.transform.rotation.w\n",
    "    print ('xyz_map', xyz_map)\n",
    "    tf_static_broad.sendTransform(static_ts)\n",
    "    return closest_centroid_height, closest_centroid_index\n",
    "\n",
    "###functions to move omnibase###\n",
    "def move_base_vel(vx, vy, vw):\n",
    "    twist = Twist()\n",
    "    twist.linear.x = vx\n",
    "    twist.linear.y = vy\n",
    "    twist.angular.z = vw \n",
    "    base_vel_pub.publish(twist)\n",
    "\n",
    "def move_base_time(x,y,yaw,timeout=0.2):\n",
    "    start_time = rospy.Time.now().to_sec()\n",
    "    while rospy.Time.now().to_sec() - start_time < timeout:  \n",
    "        move_base_vel(x, y, yaw)\n",
    "\n",
    "def tiny_move_base(velX = 0, velY = 0, velT = 0, std_time = 0.5, MAX_VEL = 0.03):\n",
    "    if abs(velX) > MAX_VEL: \n",
    "        velX =  MAX_VEL * (velX / abs(velX))\n",
    "    if abs(velY) > MAX_VEL:\n",
    "        velY = MAX_VEL * (velY / abs(velY))\n",
    "    move_base_time(velX, velY, velT, std_time)\n",
    "\n",
    "##################################\n",
    "\n",
    "            \n",
    "            ########## Functions for takeshi states ##########\n",
    "class Proto_state(smach.State):###example of a state definition.\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('State : PROTO_STATE')\n",
    "\n",
    "        if self.tries==3:\n",
    "            self.tries=0 \n",
    "            return'tries'\n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "        global trans_hand\n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            self.tries=0 \n",
    "            return'tries'\n",
    "   \n",
    "        \n",
    "\n",
    "    ##### Define state INITIAL #####\n",
    "#Estado inicial de takeshi, neutral\n",
    "class Initial(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'],input_keys=['global_counter'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        \n",
    "        rospy.loginfo('STATE : robot neutral pose')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            return 'tries'\n",
    "        try:\n",
    "            clear_octo_client()\n",
    "        except:\n",
    "            print('cant clear octomap')\n",
    "        stopper.call()\n",
    "#         scene.remove_world_object()\n",
    "        #Takeshi neutral\n",
    "        arm.set_named_target('go')\n",
    "        arm.go()\n",
    "        head.set_named_target('neutral')\n",
    "        succ = head.go()\n",
    "#         gripper.command(1.2) \n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "class Find_AR_marker(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('State : Find AR marker ')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            return 'tries'\n",
    "        try:\n",
    "            starter.call()\n",
    "            clear_octo_client()\n",
    "        except:\n",
    "            print('cant clear octomap')\n",
    "        #Takeshi looks for AR marker\n",
    "        \n",
    "        rospy.sleep(0.1)\n",
    "        hcp = head.get_current_joint_values()\n",
    "        hcp[0] = 0.5\n",
    "        hcp[1] = -0.2\n",
    "        head.set_joint_value_target(hcp)\n",
    "        head.go()\n",
    "        succ = False\n",
    "        flag = True\n",
    "        talk(\"I am going to find any AR marker\")\n",
    "        rospy.sleep(0.3)\n",
    "        while not succ:\n",
    "            try:\n",
    "                t = tfbuff.lookup_transform('base_link', 'ar_marker/201', rospy.Time(0) )\n",
    "                rospy.sleep(0.3)\n",
    "                trans, _ = tf2_obj_2_arr(t)\n",
    "                distanceX = trans[0]\n",
    "                print(distanceX)\n",
    "                flag = not flag\n",
    "                if distanceX < 0.60 and distanceX > 0.55 and flag:\n",
    "                    hcp = head.get_current_joint_values()\n",
    "                    hcp[0] += 0.4\n",
    "                    hcp[1] = -0.2\n",
    "                    head.set_joint_value_target(hcp)\n",
    "                    head.go()\n",
    "                if distanceX < 0.45:\n",
    "                    succ = True\n",
    "                else:\n",
    "                    tiny_move_base(velX=0.6,std_time=0.1)\n",
    "            except:\n",
    "                tiny_move_base(velX=0.5,std_time=0.1)\n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "class AR_alignment(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('State : pre grasp pose ')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            return 'tries'\n",
    "        # intento de vuelta\n",
    "        succ = False\n",
    "        THRESHOLD = 0.09\n",
    "        hcp = [0.6,-0.1]\n",
    "        flag = True\n",
    "        talk(\"I am going to align with the table\")\n",
    "        while not succ:\n",
    "            try:\n",
    "                t = tfbuff.lookup_transform('base_link','ar_marker/201',rospy.Time(0))\n",
    "                _, rot = tf2_obj_2_arr(t)\n",
    "#     print(rot)\n",
    "                euler = tf.transformations.euler_from_quaternion(rot)\n",
    "                theta = euler[2]\n",
    "                e = theta + 1.57\n",
    "                print(e)\n",
    "#                 if abs(e) < 1.5 and abs(e)>0.6:\n",
    "#                     hcp[0] = 0.3\n",
    "#                     head.set_joint_value_target(hcp)\n",
    "#                     head.go()\n",
    "                if abs(e) < THRESHOLD:\n",
    "                    talk(\"ready\")\n",
    "                    succ = True\n",
    "#                     hcp[0] = 0.4\n",
    "#                     head.set_joint_value_target(hcp)\n",
    "#                     head.go()\n",
    "                else:\n",
    "                    rospy.sleep(0.55)\n",
    "                    tiny_move_base(velT = 0.2*e, std_time=0.2)\n",
    "                    flag = not flag\n",
    "                    if flag:\n",
    "                        hcp[0] -= 0.1 * (e/abs(e))\n",
    "                        head.set_joint_value_target(hcp)\n",
    "                        head.go()\n",
    "            except:\n",
    "                hcp[0] -= 0.2\n",
    "                if hcp[0] > -1.2:\n",
    "                    hcp[0] = 0.0\n",
    "                head.set_joint_value_target(hcp)\n",
    "                head.go()\n",
    "                \n",
    "        \n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "\n",
    "class Pre_grasp_pose(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('State : pre grasp pose ')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            return 'tries'\n",
    "#         clear_octo_client()\n",
    "#         scene.remove_world_object()\n",
    "        #static_tf_publish_furniture(-0.95,-0.5,0.0)\n",
    "        # intento de vuelta\n",
    "        succ = False\n",
    "        talk(\"I will reach the cassette\")\n",
    "        open_gripper()\n",
    "        grasp_from_above_joints=[0.59,-1.3376,0,-1.8275,0.0,0.0]\n",
    "        arm.set_joint_value_target(grasp_from_above_joints)\n",
    "        succ = arm.go()\n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "        \n",
    "class AR_adjustment(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('State : GOTO_SHELF')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3: \n",
    "            return'tries'\n",
    "        try:\n",
    "            clear_octo_client()\n",
    "        except:\n",
    "            print('cant clear octomap')\n",
    "#         scene.remove_world_object()\n",
    "        #Takeshi gets close to the shelf\n",
    "        starter.call()\n",
    "        succ = False\n",
    "        X_OFFSET = 0.0\n",
    "        Y_OFFSET = 0.19\n",
    "        Z_OFFSET = 0.135\n",
    "\n",
    "        THRESHOLD = 0.025\n",
    "\n",
    "        hcp = head.get_current_joint_values()\n",
    "        hcp[0] = -0.1\n",
    "        hcp[1] = -0.5\n",
    "        head.set_joint_value_target(hcp)\n",
    "        head.go()\n",
    "        succ = False\n",
    "        while not succ:\n",
    "            try:\n",
    "                t = tfbuff.lookup_transform('hand_palm_link', 'ar_marker/201', rospy.Time(0) )\n",
    "#         t = tfbuff.lookup_transform('hand_palm_link', 'ar_marker/4000', rospy.Time(0) )\n",
    "                traf = t.transform.translation\n",
    "                rospy.sleep(.6)\n",
    "        # tiny_move_base(y = 0.163)\n",
    "                ex = traf.x + X_OFFSET\n",
    "                ey = -traf.y + Y_OFFSET\n",
    "                print(ex, ey)\n",
    "                if abs(ex) > THRESHOLD:\n",
    "                    tiny_move_base(velX = ex)#, y = -traf.y + Y_OFFSET)\n",
    "                if abs(ey) > THRESHOLD:\n",
    "                    tiny_move_base(velY = ey)\n",
    "                if (abs(ex) <= THRESHOLD and abs(ey) <= THRESHOLD):\n",
    "                    hcp[0] = 0\n",
    "                    head.set_joint_value_target(hcp)\n",
    "                    head.go()\n",
    "                    talk(\"I am almost there\")\n",
    "                    succ = True\n",
    "            except:\n",
    "                hcp = head.get_current_joint_values()\n",
    "                hcp[0] -= 0.1   \n",
    "                print(hcp[0])\n",
    "                head.set_joint_value_target(hcp)\n",
    "                head.go()\n",
    "                if hcp[0] < -1:\n",
    "                    hcp[0] = 0.1\n",
    "                    head.set_joint_value_target(hcp)\n",
    "                    head.go()\n",
    "                    print('Ive lost the reference')\n",
    "                    succ = False\n",
    "                    break\n",
    "        if succ:\n",
    "            stopper.call()\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "class Color_adjustment(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('State : color adjustment')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3: \n",
    "            return'tries'\n",
    "        try:\n",
    "            clear_octo_client()\n",
    "        except:\n",
    "            print('cant clear octomap')\n",
    "#         scene.remove_world_object()\n",
    "        #Takeshi scans the shelf\n",
    "        succ = False\n",
    "        THRESHOLD = 15\n",
    "        goalPos = [258.61,261.75]\n",
    "        while not succ:\n",
    "            [currentPos] = color_segmentator()\n",
    "#     print(currentPos)\n",
    "            ex = -(goalPos[0]-currentPos[0]) \n",
    "            ey = (goalPos[1]-currentPos[1])\n",
    "            print(ex, ey)\n",
    "            if abs(ex) > THRESHOLD:\n",
    "                tiny_move_base(velX = ex, std_time=0.1, MAX_VEL=0.01)#, y = -traf.y + Y_OFFSET)\n",
    "                rospy.sleep(0.5)\n",
    "            if abs(ey) > THRESHOLD:\n",
    "                tiny_move_base(velY = ey, std_time=0.1, MAX_VEL=0.01)\n",
    "                rospy.sleep(0.5)\n",
    "            if (abs(ex) <= THRESHOLD and abs(ey) <= THRESHOLD):\n",
    "                talk(\"done, now i will take it\")\n",
    "                succ = True\n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "\n",
    "    \n",
    "class Grasp_table(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'],input_keys=['global_counter'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('STATE : robot neutral pose')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            return 'failed'\n",
    "#         scene.remove_world_object()\n",
    "        #Takeshi neutral\n",
    "        open_gripper()\n",
    "        rospy.sleep(0.3)\n",
    "        acp = [0.56,-1.3376,0,-1.8275,0.0,0.0]\n",
    "        arm.set_joint_value_target(acp)\n",
    "        arm.go()\n",
    "        close_gripper()\n",
    "        rospy.sleep(0.3)\n",
    "#         gripper.command(0.0) #close\n",
    "        \n",
    "        check_grasp_joints=[0.69,-1.3376,0,-1.8275,0.0,0.0]\n",
    "        arm.set_joint_value_target(check_grasp_joints)\n",
    "        arm.go()\n",
    "        \n",
    "        check_grasp_joints=[0.69,-1.3376,-0.8,-1.8275,0.0,0.0]\n",
    "        arm.set_joint_value_target(check_grasp_joints)\n",
    "        arm.go()\n",
    "\n",
    "        THRESHOLD = 30\n",
    "        goalPos = [233.80,268.74]\n",
    "        [currentPos] = color_segmentator()\n",
    "        ex = -(goalPos[0]-currentPos[0]) \n",
    "        ey = (goalPos[1]-currentPos[1])\n",
    "        if (abs(ex) <= THRESHOLD and abs(ey) <= THRESHOLD):\n",
    "            talk(\"I have the cassete\")\n",
    "            return 'succ'\n",
    "        else:\n",
    "            talk(\"Something went wrong, i will try again\")\n",
    "            return 'tries'\n",
    "        \n",
    "class Post_grasp_pose(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'],input_keys=['global_counter'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('STATE : robot neutral pose')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            return 'tries'\n",
    "        ##Post grasp pose\n",
    "        acp = arm.get_current_joint_values()\n",
    "        acp[0] = 0.69\n",
    "        arm.set_joint_value_target(acp)\n",
    "        arm.go()\n",
    "        rospy.sleep(0.3)\n",
    "        tiny_move_base(velX = -0.6, std_time=0.9)\n",
    "        arm.set_named_target('go')\n",
    "        succ = arm.go()\n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "        \n",
    "class Right_shift(smach.State):\n",
    "    def __init__(self):\n",
    "        smach.State.__init__(self,outcomes=['succ','failed','tries'],input_keys=['global_counter'])\n",
    "        self.tries=0\n",
    "    def execute(self,userdata):\n",
    "        rospy.loginfo('STATE : robot neutral pose')\n",
    "        print('Try',self.tries,'of 5 attepmpts') \n",
    "        self.tries+=1\n",
    "        if self.tries==3:\n",
    "            return 'tries'\n",
    "#         scene.remove_world_object()\n",
    "        #Takeshi neutral\n",
    "        tiny_move_base(velY = -0.8, std_time=0.8)\n",
    "        succ = True\n",
    "        if succ:\n",
    "            return 'succ'\n",
    "        else:\n",
    "            return 'failed'\n",
    "        \n",
    "#Initialize global variables and node\n",
    "def init(node_name):\n",
    "\n",
    "    global lis, broad, tf_static_broad, tfbuff,scene, rgbd, head, whole_body, hand_cam\n",
    "    global arm, goal, navclient, clear_octo_client, service_client, base_vel_pub, starter, stopper\n",
    "\n",
    "    moveit_commander.roscpp_initialize(sys.argv)\n",
    "    rospy.init_node('Grab_cassette')\n",
    "    head = moveit_commander.MoveGroupCommander('head')\n",
    "#     gripper = moveit_commander.MoveGroupCommander('gripper')\n",
    "    whole_body = moveit_commander.MoveGroupCommander('whole_body_light')\n",
    "    arm =  moveit_commander.MoveGroupCommander('arm')\n",
    "    \n",
    "    tfbuff = tf2_ros.Buffer()\n",
    "    lis = tf2_ros.TransformListener(tfbuff)\n",
    "    broad = tf2_ros.TransformBroadcaster()\n",
    "    tf_static_broad = tf2_ros.StaticTransformBroadcaster()\n",
    "    whole_body.set_workspace([-6.0, -6.0, 6.0, 6.0]) \n",
    "    \n",
    "    scene = moveit_commander.PlanningSceneInterface()\n",
    "    robot = moveit_commander.RobotCommander()\n",
    "#     robot = Robot()\n",
    "#     gripper = robot.get('gripper')\n",
    "    \n",
    "    rgbd = RGBD()\n",
    "    hand_cam = HandRGB()\n",
    "    goal = MoveBaseGoal()\n",
    "    \n",
    "    navclient = actionlib.SimpleActionClient('/move_base/move', MoveBaseAction)\n",
    "    clear_octo_client = rospy.ServiceProxy('/clear_octomap', Empty)\n",
    "    service_client = rospy.ServiceProxy('/segment_2_tf', Trigger)\n",
    "    base_vel_pub = rospy.Publisher('/hsrb/command_velocity', Twist, queue_size=10)\n",
    "    starter = rospy.ServiceProxy('/marker/start_recognition',Empty)\n",
    "    stopper = rospy.ServiceProxy('/marker/stop_recognition',Empty)\n",
    "#     arm.set_max_velocity_scaling_factor\n",
    "    head.set_planning_time(0.3)\n",
    "    head.set_num_planning_attempts(1)\n",
    "#     gripper.set_planning_time(0.3)\n",
    "#     gripper.set_num_planning_attempts(1)\n",
    "\n",
    "#Entry point    \n",
    "if __name__== '__main__':\n",
    "    print(\"Takeshi STATE MACHINE...\")\n",
    "    init(\"takeshi_smach_20\")\n",
    "    sm = smach.StateMachine(outcomes = ['END'])     #State machine, final state \"END\"\n",
    "\n",
    "    with sm:\n",
    "        #State machine for grasping on Table\n",
    "        smach.StateMachine.add(\"INITIAL\",Initial(),transitions = {'failed':'INITIAL', 'succ':'FIND_AR_MARKER', 'tries':'END'}) \n",
    "        smach.StateMachine.add(\"FIND_AR_MARKER\",Find_AR_marker(),transitions = {'failed':'END', 'succ':'AR_ALIGNMENT', 'tries':'FIND_AR_MARKER'}) \n",
    "        smach.StateMachine.add(\"AR_ALIGNMENT\",AR_alignment(),transitions = {'failed':'AR_ALIGNMENT', 'succ':'PRE_GRASP_POSE', 'tries':'AR_ALIGNMENT'}) \n",
    "        smach.StateMachine.add(\"PRE_GRASP_POSE\",Pre_grasp_pose(),transitions = {'failed':'RIGHT_SHIFT', 'succ':'AR_ADJUSTMENT', 'tries':'PRE_GRASP_POSE'}) \n",
    "        smach.StateMachine.add(\"RIGHT_SHIFT\",Right_shift(),transitions = {'failed':'RIGHT_SHIFT', 'succ':'PRE_GRASP_POSE', 'tries':'PRE_GRASP_POSE'}) \n",
    "        smach.StateMachine.add(\"AR_ADJUSTMENT\",AR_adjustment(),transitions = {'failed':'END', 'succ':'COLOR_ADJUSTMENT', 'tries':'COLOR_ADJUSTMENT'}) \n",
    "        smach.StateMachine.add(\"COLOR_ADJUSTMENT\",Color_adjustment(),transitions = {'failed':'END', 'succ':'GRASP_TABLE', 'tries':'END'})\n",
    "        smach.StateMachine.add(\"GRASP_TABLE\",Grasp_table(),transitions = {'failed':'END', 'succ':'POST_GRASP_POSE', 'tries':'GRASP_TABLE'})\n",
    "        smach.StateMachine.add(\"POST_GRASP_POSE\",Post_grasp_pose(),transitions = {'failed':'END', 'succ':'END', 'tries':'GRASP_TABLE'})\n",
    "      \n",
    "\n",
    "    outcome = sm.execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61b095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
